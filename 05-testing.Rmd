# Testing best practise

**Author: Harsh Jain**  

This chapter is about best practises for testing or Test-driven development (TDD)

It is important to note that Test-driven development (TDD) is not solely a testing technique, but rather part of a holistic design, development and testing process. The basic idea of TDD is that instead of writing your code first and then writing tests after to ensure the code works, you write your tests first and then write the code that will get all the tests to pass. This is known as a Test-First approach.

There are two generally accepted views on how and why you should practice TDD in your software development. 

* The first view sees TDD as a technique for specifying your requirements and design before writing the actual program code. 

* The second view takes a more pragmatic approach and sees TDD as a technique that helps programmers write better code. 

Regardless of the view one takes, what TDD practitioners all agree on is that TDD will not only improve your code, but it will also improve the overall design and implementation of your software system. Below are two types of test which are important for us

1. **Unit Test**: verifies a single logic about the unit of work 
2. **Integration Test**: verifies the connection and data flow  between components

### Testing Framework/Liberies

 * **Python**: unittest, pytest, pymock, coverage
 * **R**: testthat, RUnit, svUnit

## Test structure

Effective layout of a test case ensures all required actions are completed, improves the readability of the test case, and smooths the flow of execution. Consistent structure helps in building a self-documenting test case. A commonly applied structure for test cases has (1) setup, (2) execution, (3) validation, and (4) cleanup.

* **Setup**: Put the Unit Under Test (UUT) or the overall test system in the state needed to run the test.
* **Execution**: Trigger/drive the UUT to perform the target behaviour and capture all output, such as return values and output parameters. This step is usually very simple.
* **Validation**: Ensure the results of the test are correct. These results may include explicit outputs captured during execution or state changes in the UUT.
* **Cleanup**: Restore the UUT or the overall test system to the pre-test state. This restoration permits another test to execute immediately after this one.

## Naming Conventions
Naming conventions help organize tests better so that it is easier for developers to find what they’re looking for. Another benefit is that many tools expect that those conventions are
followed. 

* **Separate the implementation from the test code**: It avoids accidentally packaging tests together with production binaries; many build tools expect tests to be in a certain source directory.

* **Place test classes in the same package as implementation**: It helps finding tests, knowing that tests are in the same package as the code they test helps finding them faster. 

* **Name test classes in a similar fashion as classes they test**: It helps finding tests. One commonly used practice is to name tests the same as implementation classes with suffix Test. If, for example, implementation class is UserController, test class should be UserControllerTest. Often, number of lines in test classes is bigger than number of lines in corresponding implementation class. There can be many test methods for each implementation method. To help locate methods that are tested, test classes can be split. For example, if StringCalculator has methods createUser and deleteUser, there can be test classes StringCalculatorAddTest and StringCalculatorRemoveTest.

* **Use descriptive names for test methods**: It helps understanding the objective of tests. Using method names that describe tests is beneficial when trying to figure out why some test failed or when the coverage should be increased with more tests. It should be clear what conditions are set before the test, what actions are performed and what is the expected outcome. There are many different ways to name test methods. Our preferred method is to name them using the Given/When/Then syntax used in BDD scenarios. Given describes (pre)conditions, When describes actions and Then describes the expected outcome. If some test does not have preconditions (usually set using @Before and @BeforeClass annotations), Given can
be skipped

## Processes
TDD processes are the core set of practices. Successful implementation of TDD depends on practices described in this section.

* **Write the test before writing the implementation code**: This ensures that testable code is written; ensures that every line of code gets tests written for it.

* **Only write new code when test is failing**: It confirms that the test does not work without the implementation. If tests are passing without the need to write or modify the implementation code then either the functionality is already implemented or test is defective. If new functionality is indeed
missing then test always passes and is therefore useless. Test should fail for the expected reason. Even though there are no guarantees that test is verifying the right thing, with fail
first and for the expected reason, confidence that verification is correct should be high.

* **Integrate running tests in build pipeline**: If your project has a continuous integration(Jenkins) implemented then  ensure that as soon as you commit your changes to VCS(git) a build is triggered and runs all the test cases


## Miscellaneous “Do”s
* Separate common set-up and teardown logic into test support services utilized by the appropriate test cases.
* Keep each test focused on only the results necessary to validate its test.
* Is very limited in scope. If the test fails, it's obvious where to look for the problem. Use few Assert calls so that the offending code is obvious. It's important to only test one thing in a single test.
* Runs fast, runs fast, runs fast. If the tests are slow, they will not be run often.
* Clearly reveals its intention. Another developer can look at the test and understand what is expected of the production code.
* Runs and passes in isolation. If the tests require special environmental setup or fail unexpectedly, then they are not good unit tests. Change them for simplicity and reliability. Tests should run and pass on any machine. The "works on my system" excuse doesn't work.
* Often uses stubs and mock objects. If the code being tested typically calls out to a database or file system, these dependencies must be simulated, or mocked. These dependencies will ordinarily be abstracted away by using interfaces.
* Design time-related tests to allow tolerance for execution in non-real time operating systems. The common practice of allowing a 5-10 percent margin for late execution reduces the potential number of false negatives in test execution.
* Test the coverage of your code base and aim for maximum coverage various tools and plugins can be used depending on IDE and programming language of choice.
* Treat your test code with the same respect as your production code. It also must work correctly for both positive and negative cases, last a long time, and be readable and maintainable.

## Miscellaneous “Don’t”s
* Having test cases depend on system state manipulated from previously executed test cases (i.e., you should always start a unit test from a known and pre-configured state).
* Dependencies between test cases. A test suite where test cases are dependent upon each other is brittle and complex. Execution order should not be presumed. Basic refactoring of the initial test cases or structure of the UUT causes a spiral of increasingly pervasive impacts in associated tests.
* Testing precise execution behaviour timing or performance.
Building "all-knowing oracles". An oracle that inspects more than necessary is more expensive and brittle over time. This very common error is dangerous because it causes a subtle but pervasive time sink across the complex project.
* Slow running tests.
